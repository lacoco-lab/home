% BibTeX bibliography file


@inproceedings{rathi-information-2021,
    title = "An Information-Theoretic Characterization of Morphological Fusion",
    author = "Rathi, Neil  and
      Michael Hahn  and
      Futrell, Richard",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    github = {https://github.com/neilrathi/morphological-fusion},
    month = nov,
    png = {figs/rathi2021.png},
    year = "2021",
    url = "https://aclanthology.org/2021.emnlp-main.793",
    pages = "10115--10120",
    abstract = "Linguistic typology generally divides synthetic languages into groups based on their morphological fusion. However, this measure has long been thought to be best considered a matter of degree. We present an information-theoretic measure, called informational fusion, to quantify the degree of fusion of a given set of morphological features in a surface form, which naturally provides such a graded scale. Informational fusion is able to encapsulate not only concatenative, but also nonconcatenative morphological systems (e.g. Arabic), abstracting away from any notions of morpheme segmentation. We then show, on a sample of twenty-one languages, that our measure recapitulates the usual linguistic classifications for concatenative systems, and provides new measures for nonconcatenative ones. We also evaluate the long-standing hypotheses that more frequent forms are more fusional, and that paradigm size anticorrelates with degree of fusion. We do not find evidence for the idea that languages have characteristic levels of fusion; rather, the degree of fusion varies across part-of-speech within languages.",
}

@inproceedings{hewitt-rnns-2020,
  author    = {John Hewitt and Michael Hahn and Surya Ganguli and Percy Liang and Christopher Manning},
  title     = {{RNN}s can generate bounded hierarchical languages with optimal memory},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020)},
  year      = {2020},
  pages={1978--2010},
  month = {January},
  png = {figs/rnns.png},
  url = {https://www.aclweb.org/anthology/2020.emnlp-main.156.pdf},
  preprint = {https://arxiv.org/abs/2010.07515},
  github = {https://github.com/john-hewitt/dyckkm-constructions/}
}

@INPROCEEDINGS{Hahn-etal-2019-arxiv,
  author    = {Michael Hahn and
               Frank Keller and Yonatan Bisk and Yonatan Belinkov},
  title     = {Character-based surprisal as a model of human reading in the presence of errors},
  png = {figs/errors.png},
  booktitle = {Proceedings of the 41st Annual Meeting of the Cognitive Science Society (CogSci)},
  year      = {2019},
  url = {https://cogsci.mindmodeling.org/2019/papers/0089/0089.pdf},
  preprint       = {https://arxiv.org/abs/1902.00595},
  github = {https://github.com/m-hahn/reading-noise},
  archivePrefix = {arXiv},
  eprint    = {1902.00595},
  abstract = {Intuitively, human readers cope easily with errors in text; typos, misspelling, word substitutions, etc. do not unduly disrupt natural reading. Previous work indicates that letter transpositions result in increased reading times, but it is unclear if this effect generalizes to more natural errors. In this paper, we report an eye-tracking study that compares two error types (letter transpositions and naturally occurring misspelling) and two error rates (10% or 50% of all words contain errors). We find that human readers show unimpaired comprehension in spite of these errors, but error words cause more reading difficulty than correct words. Also, transpositions are more difficult than misspellings, and a high error rate increases difficulty for all words, including correct ones. We then present a computational model that uses character-based (rather than traditional word-based) surprisal to account for these results. The model explains that transpositions are harder than misspellings because they contain unexpected letter combinations. It also explains the error rate effect: upcoming words are more difficultto predict when the context is degraded, leading to increased surprisal.}
}


@article{hahn2020sensitivity,
  author    = {Michael Hahn and
               Dan Jurafsky and Richard Futrell},
  title     = {Sensitivity as a complexity measure for sequence classification tasks},
  png = {figs/sensirivity.png},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {9},
  pages = {891--908},
  year      = {2021},
  month = {November},
  url = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00403/106992/Sensitivity-as-a-Complexity-Measure-for-Sequence},
  preprint       = {https://arxiv.org/abs/2104.10343},
  github = {https://github.com/m-hahn/sensitivity},
  slides = {files/sensitivity-slides.pdf}
}




@article{Hahn2020modeling,
  author    = {Michael Hahn and
               Judith Degen and Richard Futrell},
  title     = {Modeling word and morpheme order in natural language as an efficient tradeoff of memory and surprisal},
  journal   = {Psychological Review},
  png = {figs/memsuro.png},
  volume={128},
  issue={4},
  month = {December},
  pages={726--756},
  year      = {2021},
  preprint       = {https://psyarxiv.com/nu4qz},
  archivePrefix = {psyarxiv},
  doi = {10.1037/rev0000269},
  url = {files/hahn_psychreview_2021_final.pdf},
  github = {https://github.com/m-hahn/memory-surprisal},
  supplement = {files/hahn_psychreview_2021_si.pdf},
  slides = {files/osf-CUNY 2019-Tradeoff.pdf},
  abstract = {Memory limitations are known to constrain language comprehension and production, and have been argued to account for crosslinguistic word order regularities.
However, a systematic assessment of the role of memory limitations in language structure has proven elusive, in part because it is hard to extract precise large-scale quantitative generalizations about language from existing mechanistic models of memory use in sentence processing.
We provide an architecture-independent information-theoretic formalization of memory limitations which enables a simple calculation of the memory efficiency of languages.
Our notion of memory efficiency is based on the idea of a memory--surprisal tradeoff: a certain level of average surprisal per word can only be achieved at the cost of storing some amount of information about past context.
Based on this notion of memory usage, we advance the Efficient Tradeoff Hypothesis: the order of elements in natural language is under pressure to enable favorable memory-surprisal tradeoffs.
We derive that languages enable more efficient tradeoffs when they exhibit information locality: when predictive information about an element is concentrated in its recent past.
We provide empirical evidence from three test domains in support of the Efficient Tradeoff Hypothesis:
a reanalysis of a miniature artificial language learning experiment, a large-scale study of word order in corpora of 54 languages, and an analysis of morpheme order in two agglutinative languages. These results suggest that principles of order in natural language can be explained via highly generic cognitively motivated principles and lend support to efficiency-based models of the structure of human language.}
}




@article{Hahn-Keller-2018-arxiv,
  author    = {Michael Hahn and
               Frank Keller},
  title     = {Modeling task effects in human reading with neural network-based attention},
  journal   = {Cognition},
  png={figs/cognition2023},
  year      = {2023},
  volume = {230},
  pages = {105289},
  preprint       = {https://arxiv.org/abs/1808.00054},
  archivePrefix = {arXiv},
  eprint    = {1808.00054},
  github = {https://gitlab.com/m-hahn/task-effects-neural-networks/},
  month={july},
  URL = {https://www.sciencedirect.com/science/article/pii/S0010027722002773}
}

@article{Hahn-2019-arxiv,
author = {Michael Hahn},
title = {Theoretical limitations of self-attention in neural sequence models},
journal = {Transactions of the Association for Computational Linguistics},
volume = {8},
number = {},
pages = {156-171},
year = {2020},
month = {October},
doi = {10.1162/tacl\_a\_00306},
png = {figs/transformers.png},
URL = { 
        https://doi.org/10.1162/tacl_a_00306
},
	slides={files/acl2020-selfattention.pdf},
  preprint = {https://arxiv.org/abs/1906.06755},
  supplement = {files/transformers-proof.pdf},
,
    abstract = { Transformers are emerging as the new workhorse of NLP, showing great success across tasks. Unlike LSTMs, transformers process input sequences entirely through self-attention. Previous work has suggested that the computational capabilities of self-attention to process hierarchical structures are limited. In this work, we mathematically investigate the computational power of self-attention to model formal languages. Across both soft and hard attention, we show strong theoretical limitations of the computational abilities of self-attention, finding that it cannot model periodic finite-state languages, nor hierarchical structure, unless the number of layers or heads increases with input length. These limitations seem surprising given the practical success of self-attention and the prominent role assigned to hierarchical structure in linguistics, suggesting that natural language can be approximated well with models that are too weak for the formal languages typically assumed in theoretical linguistics. }
}







@INPROCEEDINGS{Hahn-Krebs-Straubing-2018,
  author = {Michael Hahn and Andreas Krebs and Howard Straubing},
  title = {Wreath Products of Distributive Forest Algebras},
  booktitle = {Logic in Computer Science (LICS)},
  png = {figs/lics.png},
  year = {2018},
  URL = {files/lics-2018-submitted.pdf},
  github = {https://github.com/m-hahn/lics2018-wreath-products},
  preprint = {https://arxiv.org/abs/1911.03493},
  slides = {https://www.mhahn.info/files/lics2018.pdf},
  abstract = {It is an open problem whether definability in Propositional Dynamic Logic (PDL) on forests is decidable. Based on an algebraic characterization by Bojańczyk, et. al.,(2012) in terms of forest algebras, Straubing (2013) described an approach to PDL based on a k-fold iterated distributive law. A proof that all languages satisfying such a k-fold iterated distributive law are in PDL would settle decidability of PDL. We solve this problem in the case k=2: All languages recognized by forest algebras satisfying a 2-fold iterated distributive law are in PDL. Furthermore, we show that this class is decidable. This provides a novel nontrivial decidable subclass of PDL, and demonstrates the viability of the proposed approach to deciding PDL in general.}
}

@INPROCEEDINGS{Hahn_information_2018,
  author = {Michael Hahn and Judith Degen and Noah Goodman and Dan Jurafsky and Richard Futrell},
  title = {An information-theoretic explanation of adjective ordering preferences},
  booktitle = {Proceedings of the 40th Annual Meeting of the Cognitive Science Society (CogSci)},
  png = {figs/cogsci.png},
  year = {2018},
  pages={1766--1771},
  URL = {https://cogsci.mindmodeling.org/2018/papers/0339/index.html},
  abstract = {Across languages, adjectives are subject to ordering restrictions. Recent research shows that these are predicted by adjective subjectivity, but the question remains open why this is the case. We first conduct a corpus study and not only replicate the subjectivity effect, but also find a previously undocumented effect of mutual information between adjectives and nouns. We then describe a rational model of adjective use in which listeners explicitly reason about judgments made by different speakers, formalizing the notion of subjectivity as agreement between speakers. We show that, once incremental processing is combined with memory limitations, our model predicts effects both of subjectivity and mutual information. We confirm the adequacy of our model by evaluating it on corpus data, finding that it correctly predicts ordering in unseen data with an accuracy of 96.2 %. This suggests that adjective ordering can be explained by general principles of human communication and language processing.},
 github = {https://github.com/m-hahn/adjective-ordering-model.git},
 slides = {https://www.mhahn.info/files/AdjOrdTexmod21Final.pdf}
}



@InProceedings{hahn_arabic_2012,
  author = {Michael Hahn},
  title = {Arabic {Relativization} {Patterns}: {A} {Unified} {HPSG}
  {Analysis}},
  booktitle = {Proceedings of the 19th {International} {Conference} on
  {Head}-{Driven} {Phrase} {Structure} {Grammar}},
  year = {2012},
  address = {Chungnam National University Daejeon},
  URL =
  {http://web.stanford.edu/group/cslipublications/cslipublications/HPSG/2012/hahn.pdf}
}

@InProceedings{hahn_deriving_2011,
  author = {Michael Hahn and Meurers, Detmar},
  title = {On deriving semantic representations from dependencies: {A}
  practical approach for evaluating meaning in learner corpora},
  booktitle = {Proceedings of the {Int}. {Conference} on {Dependency}
  {Linguistics} ({Depling} 2011)},
  year = {2011},
  pages = {94--103},
  URL = {http://purl.org/dm/papers/hahn-meurers-11.html}
}

@InProceedings{hahn_deriving_2014,
  author = {Michael Hahn and Meurers, Detmar},
  title = {On deriving semantic representations from dependencies: {A}
  practical approach for evaluating meaning in learner corpora},
  booktitle = {Dependency {Theory}},
  editor = {Gerdes, Kim and Hajicov{\'a}, Eva and Wanner, Leo},
  year = {2014},
  series = {Frontiers in {AI} and {Applications} {Series}},
  publisher = {IOS Press}
}

@InProceedings{hahn_evaluating_2012,
  author = {Michael Hahn and Meurers, Detmar},
  title = {Evaluating the {Meaning} of {Answers} to {Reading} {Comprehension}
  {Questions}: {A} {Semantics}-{Based} {Approach}},
  booktitle = {Proceedings of the 7th {Workshop} on {Innovative} {Use} of {NLP}
  for {Building} {Educational} {Applications} ({BEA}7)},
  year = {2012},
  publisher = {Association for Computational Linguistics},
  URL = {http://aclweb.org/anthology/W12-2039.pdf}
}

@Article{hahn_henkin_2015,
  author = {Michael Hahn and Richter, Frank},
  title = {Henkin {Semantics} for {Reasoning} with {Natural} {Language}},
  journal = {Journal of Language Modeling},
  year = {2015},
  volume = {3},
  number = {2},
  pages = {513--568},
  png = {figs/henkin.png},
  URL = {http://jlm.ipipan.waw.pl/index.php/JLM/article/view/113},
  github = {https://github.com/m-hahn/henkin},
  abstract = {The frequency of intensional and non-first-order definable operators in natural languages constitutes a challenge for automated reasoning with the kind of logical translations that are deemed adequate by formal semanticists. Whereas linguists employ expressive higher-order logics in their theories of meaning, the most successful logical reasoning strategies with natural language to date rely on sophisticated first-order theorem provers and model builders. In order to bridge the fundamental mathematical gap between linguistic theory and computational practice, we present a general translation from a higher-order logic frequently employed in the linguistics literature, two-sorted Type Theory, to first-order logic under Henkin semantics. We investigate alternative formulations of the translation, discuss their properties, and evaluate the availability of linguistically relevant inferences with standard theorem provers in a test suite of inference problems stated in English. The results of the experiment indicate that translation from higher-order logic to first-order logic under Henkin semantics is a promising strategy for automated reasoning with natural languages.}
}

@Article{hahn2019tabula,
   author = {Michael Hahn and Marco Baroni},
   title = {Tabula nearly rasa: Probing the linguistic knowledge of character-level neural language models trained on unsegmented text},
   journal = {Transactions of the Association for Computational Linguistics},
   year = {2019},
   volume = {7},
   pages = {467--484},
   github = {https://github.com/m-hahn/tabula-rasa-rnns},
   URL = {https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00283},
   preprint = {https://arxiv.org/abs/1906.07285},
   png = {figs/baroni.png},
   abstract = {Recurrent neural networks (RNNs) have reached striking performance in many natural language processing tasks. This has renewed interest in whether these generic sequence processing devices are inducing genuine linguistic knowledge. Nearly all current analytical studies, however, initialize the RNNs with a vocabulary of known words, and feed them tokenized input during training. We present a multi-lingual study of the linguistic knowledge encoded in RNNs trained as character-level language models, on input data with word boundaries removed. These networks face a tougher and more cognitively realistic task, having to discover any useful linguistic unit from scratch based on input statistics. The results show that our "near tabula rasa" RNNs are mostly able to solve morphological, syntactic and semantic tasks that intuitively presuppose word-level knowledge, and indeed they learned, to some extent, to track word boundaries. Our study opens the door to speculations about the necessity of an explicit, rigid word lexicon in language learning and usage.}
}

@Article{hahn2019estimating,
  author = {Michael Hahn and Futrell, Richard},
  title = {Estimating predictive rate-distortion curves via neural variational inference },
  journal = {Entropy},
  volume = {21},
  number = {7},
  pages = {640},
  URL = {https://www.mdpi.com/1099-4300/21/7/640},
  png = {figs/entropy.png},
  year = {2019},
  github = {https://github.com/m-hahn/predictive-rate-distortion},
  reviews = {https://www.mdpi.com/1099-4300/21/7/640/review_report},
  abstract = {The Predictive Rate–Distortion curve quantifies the trade-off between compressing information about the past of a stochastic process and predicting its future accurately. Existing estimation methods for this curve work by clustering finite sequences of observations or by utilizing analytically known causal states. Neither type of approach scales to processes such as natural languages, which have large alphabets and long dependencies, and where the causal states are not known analytically. We describe Neural Predictive Rate–Distortion (NPRD), an estimation method that scales to such processes, leveraging the universal approximation capabilities of neural networks. Taking only time series data as input, the method computes a variational bound on the Predictive Rate–Distortion curve. We validate the method on processes where Predictive Rate–Distortion is analytically known. As an application, we provide bounds on the Predictive Rate–Distortion of natural language, improving on bounds provided by clustering sequences. Based on the results, we argue that the Predictive Rate–Distortion curve is more useful than the usual notion of statistical complexity for characterizing highly complex processes such as natural language.}
}



@InProceedings{hahn_modeling_2016,
  author = {Michael Hahn and Keller, Frank},
  title = {Modeling human reading with neural attention},
  booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  year = {2016},
  png = {figs/emnlp2016.png},
  URL = {https://www.aclweb.org/anthology/D16-1009.pdf},
  github = {https://github.com/m-hahn/human-reading-neural-attention},
  abstract = {When humans read text, they fixate some words and skip others. However, there have been few attempts to explain skipping behavior with computational models, as most existing work has focused on predicting reading times (e.g.,~using surprisal). In this paper, we propose a novel approach that models both skipping and reading, using an unsupervised architecture that combines a neural attention with autoencoding, trained on raw text using reinforcement learning. Our model explains human reading behavior as a tradeoff between precision of language understanding (encoding the input accurately) and economy of attention (fixating as few words as possible). We evaluate the model on the Dundee eye-tracking corpus, showing that it accurately predicts skipping behavior and reading times, is competitive with surprisal, and captures known qualitative features of human reading.}
}

@InProceedings{hahn_null_2011,
  author = {Michael Hahn},
  title = {Null {Conjuncts} and {Bound} {Pronouns} in {Arabic}},
  booktitle = {The {Proceedings} of the 18th {International} {Conference} on
  {Head}-{Driven} {Phrase} {Structure} {Grammar}},
  year = {2011}
}

@InProceedings{hahn_predication_2014,
  author = {Michael Hahn},
  title = {Predication and {NP} {Structure} in an {Omnipredicative} {Language}:
  {The} {Case} of {Khoekhoe}},
  png = {figs/predication.png},
  booktitle = {Proceedings of the 21st {{I}}nternational {{C}}onference on
  {{H}}ead-{{D}}riven {{P}}hrase {{S}}tructure {{G}}rammar},
  editor = {{Stefan Müller}},
  year = {2014},
  publisher = {CSLI Publications},
  URL =
  {http://web.stanford.edu/group/cslipublications/cslipublications/HPSG/2014/hahn.pdf},
  abstract = {We examine noun phrases and predication in Khoekhoe, a Central Khoisan language, arguing that members of all open word classes can function equally and without derivation as predicates, and that predicative use is primary and referential use is derived syntactically by relativization.  We then present a formal HPSG analysis, in which members of all open word classes enter thesyntax as predicates and in which all argument NPs are derived in a uniform manner as projections of pronominal elements, modified by relative clauses, building on Sag's (1997) analysis of English relative clauses. We will then argue that, additionally, DPs may project directly to clauses, yielding a second predication structure.}
}

@InProceedings{hahn_visibly_2015,
  author = {Michael Hahn and Krebs, Andreas and Lange, Klaus-J{\"o}rn and
  Ludwig, Michael},
  title = {Visibly {Counter} {Languages} and the {Structure} of {NC}1},
  booktitle = {Proceedings of {Mathematical} {Foundations} of {Computer}
  {Science} ({MFCS}) 2015},
  year = {2015},
  URL = {https://doi.org/10.1007/978-3-662-48054-0_32},
  abstract = {We extend the familiar program of understanding circuit complexity in terms of regular languages to visibly counter languages. Like the regular languages, the visibly counter languages are NC1- complete. We investigate what the visibly counter languages in certain constant depth circuit complexity classes are. We have initiated this in a previous work for AC0. We present characterizations and decidability results for various logics and circuit classes. In particular, our approach yields a way to understand TC0, where the regular approach fails.}
}

@InProceedings{hahn_word_2013,
  author = {Michael Hahn},
  title = {Word {Order} {Variation} in {Khoekhoe}},
  booktitle = {Proceedings of the 20th {{I}}nternational {{C}}onference on
  {{H}}ead-{{D}}riven {{P}}hrase {{S}}tructure {{G}}rammar},
  editor = {Müller, Stefan},
  year = {2013},
  pages = {48--68},
  URL =
  {http://web.stanford.edu/group/cslipublications/cslipublications/HPSG/2013/hahn.pdf}
}

@InProceedings{ott_comet:_2013,
  author = {Ott, Niels and Ziai, Ramon and Michael Hahn and Meurers, Detmar},
  title = {{CoMeT}: {Integrating} different levels of linguistic modeling for
  meaning assessment},
  booktitle = {Proceedings of the 7th {International} {Workshop} on {Semantic}
  {Evaluation} ({SemEval})},
  year = {2013},
  pages = {608--616},
  URL = {http://aclweb.org/anthology/S13-2102.pdf}
}
@article{hahn_universals_2019,
  author = {Michael Hahn and Jurafsky, Dan and Futrell, Richard},
  title = {Universals of word order reflect optimization of grammars for efficient communication},
  png = {figs/pnas2020.png},
  pages={2347--2353},
  volume={117},
  issue={5},
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  year = {2020},
  month = {December},
  URL = {https://www.pnas.org/content/early/2020/01/16/1910923117},
  github = {https://github.com/m-hahn/grammar-optim},
  supplement = {https://www.pnas.org/content/pnas/suppl/2020/01/17/1910923117.DCSupplemental/pnas.1910923117.sapp.pdf},
  abstract = {Human languages share many grammatical properties. We show that some of these properties can be explained by the need for languages to offer efficient communication between humans given our cognitive constraints. Grammars of languages seem to find a balance between two communicative pressures: to be simple enough to allow the speaker to easily produce sentences, but complex enough to be unambiguous to the hearer, and this balance explains well-known word-order generalizations across our sample of 51 varied languages. Our results offer quantitative and computational evidence that language structure is dynamically shaped by communicative and cognitive pressures.}
}


@article{hahn-2022-morpheme,
	author = {Michael Hahn and Rebecca Mathew and Judith Degen},
	title = {Morpheme ordering across languages reflects optimization for processing efficiency},
	journal = {Open Mind: Discoveries in Cognitive Science},
	year = {2022},
    volume = {5},
        doi = {10.1162/opmi_a_00051},
	    eprint = {https://direct.mit.edu/opmi/article-pdf/doi/10.1162/opmi\_a\_00051/1986664/opmi\_a\_00051.pdf},
    pages = {208-232},
    month = {June},
	URL = {https://direct.mit.edu/opmi/article/doi/10.1162/opmi_a_00051/109033/Morpheme-Ordering-Across-Languages-Reflects},
	github={https://github.com/beckymathew/morpheme-ordering},
	supplement={files/opmi_a_00051-supplement.pdf},
	abstract = {The ordering of morphemes in a word displays well-documented regularities across languages. Previous work has explained these in terms of notions such as semantic scope, relevance, and productivity. Here, we test a recently formulated processing theory of the ordering of linguistic units, the Efficient Tradeoff Hypothesis [Hahn et al., 2021]. The claim of the theory is that morpheme ordering can partly be explained by the optimization of a tradeoff between memory and surprisal. This claim has received initial empirical support from two languages. In this work, we test this idea more extensively using data from four additional agglutinative languages with significant amounts of morphology, and by considering nouns in addition to verbs. We find that the Efficient Tradeoff Hypothesis predicts ordering in most cases with high accuracy, and accounts for cross-linguistic regularities in noun and verb inflection. Our work adds to a growing body of work suggesting that many ordering properties of language arise from a pressure for efficient language processing.},
	png = {figs/hahn2022morpheme.png}
}

@article{hahn-2022-crosslinguistic,
	author= {Michael Hahn and Yang Xu},
	title = {Crosslinguistic word order variation reflects evolutionary pressures of dependency and information locality},
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  year = {2022},
  URL = {https://www.pnas.org/doi/10.1073/pnas.2122604119},
  doi = {10.1073/pnas.2122604119},
  volume={119},
  number={24},
  pages={e2122604119},
  github = {https://gitlab.com/m-hahn/efficiency-basic-word-order/},
  preprint = {https://arxiv.org/abs/2206.04239v1},
  supplement = {https://www.pnas.org/doi/suppl/10.1073/pnas.2122604119/suppl_file/pnas.2122604119.sapp.pdf},
  month={July},
png={figs/hahn2022crosslinguistic.png}
}

@inproceedings{yan-2022-modeling,
	author={Songpeng Yan and Michael Hahn and Frank Keller},
	title={Modeling fixation behavior in reading with character-level neural attention},
	booktitle={Proceedings of the 44th Annual Meeting of the Cognitive  Science Society (CogSci)},
	year={2022},
	month={May},
	url={https://escholarship.org/uc/item/62b896xq},
	png = {figs/char-fix2.png}
}



@inproceedings{rathi-2022-explaining,
	author={Neil Rathi and Michael Hahn and Richard Futrell},
	png = {figs/fusion.png},
	title={Explaining patterns of fusion in morphological paradigms using the memory--surprisal tradeoff},
	booktitle={Proceedings of the 44th Annual Meeting of the Cognitive  Science Society (CogSci)},
	year={2022},
	month={May},
	note={Winner of the Sayan Gul Award for Best Undergraduate Paper},
	url={https://escholarship.org/uc/item/0v03z6xb}
}


@article{futrell2022information,
   author={Richard Futrell and Michael Hahn},
   png = {figs/infotheory.png},
   year={2022},
   title={Information theory as a bridge between language function and language form},
  journal={Frontiers in Communication},
 volume={7},
 issue={657725},
 doi={10.3389/fcomm.2022.657725},
 url={https://www.frontiersin.org/articles/10.3389/fcomm.2022.657725/full}
}

@article{hahn2022resource,
	title={A resource-rational model of human processing of recursive linguistic structure},
	year={2022},
        journal = {Proceedings of the National Academy of Sciences of the United States of America},
	volume={119},
	number={43},
	pages={e2122602119},
	author={Michael Hahn and Richard Futrell and Roger Levy and Edward Gibson},
	doi={https://doi.org/10.1073/pnas.2122602119},
	url={https://doi.org/10.1073/pnas.2122602119},
	month={oct},
	github={https://gitlab.com/m-hahn/resource-rational-surprisal},
        supplement = {https://www.pnas.org/doi/suppl/10.1073/pnas.2122602119/suppl_file/pnas.2122602119.sapp.pdf},
	commentary={https://www.pnas.org/doi/10.1073/pnas.2217108119},
	png={figs/hahn2022resource.png}
}

@article{hahn2022biases,
	title={A unifying theory explains seemingly contradicting biases in perceptual estimation},
	year={In Preparation},
	journal={bioRxiv Preprint},
	author={Michael Hahn and Xue-Xin Wei},
	month={february},
	png = {figs/biases.png},
	github={https://gitlab.com/m-hahn/unifying-theory-biases},
	supplement = {https://www.biorxiv.org/content/10.1101/2022.12.12.519538v1.supplementary-material},
		url={https://www.biorxiv.org/content/10.1101/2022.12.12.519538v1}
}

@article{hahn2023theory,
	title={A theory of emergent in-context learning as implicit structure induction},
	author={Michael Hahn and Navin Goyal},
	journal={arXiv Preprint},
	month={September},
	year={In Preparation},
	     url={https://arxiv.org/abs/2303.07971}
}

@article{clark2023pressure,
  author    = {Clark, Thomas Hikaru and Clara Meister and Tiago Pimentel and Michael Hahn and Ryan Cotterell and Richard Futrell and Roger Levy},
    title = "{A Cross-Linguistic Pressure for Uniform Information Density in Word Order}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {11},
    pages = {1048-1065},
    year = {2023},
    month = {08},
    abstract = "{While natural languages differ widely in both canonical word order and word order flexibility, their word orders still follow shared cross-linguistic statistical patterns, often attributed to functional pressures. In the effort to identify these pressures, prior work has compared real and counterfactual word orders. Yet one functional pressure has been overlooked in such investigations: The uniform information density (UID) hypothesis, which holds that information should be spread evenly throughout an utterance. Here, we ask whether a pressure for UID may have influenced word order patterns cross-linguistically. To this end, we use computational models to test whether real orders lead to greater information uniformity than counterfactual orders. In our empirical study of 10 typologically diverse languages, we find that: (i) among SVO languages, real word orders consistently have greater uniformity than reverse word orders, and (ii) only linguistically implausible counterfactual orders consistently exceed the uniformity of real orders. These findings are compatible with a pressure for information uniformity in the development and usage of natural languages.1}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00589},
    url = {https://doi.org/10.1162/tacl\_a\_00589},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00589/2154495/tacl\_a\_00589.pdf},
}





@String {
	hp_EdwardGibson = {https://bcs.mit.edu/directory/edward-gibson},
		hp_RogerLevy = {https://www.mit.edu/%7erplevy/},
  hp_DanJurafsky      = {https://web.stanford.edu/%7ejurafsky/},
  hp_RichardFutrell      = {http://socsci.uci.edu/%7erfutrell/},
  hp_YangXu = {https://www.cs.toronto.edu/%7eyangxu},
  hp_FrankKeller = {https://homepages.inf.ed.ac.uk/keller},
  hp_JudithDegen = {https://thegricean.github.io},
  hp_JohnHewitt = {https://nlp.stanford.edu/%7ejohnhew},
hp_ChristopherManning={https://nlp.stanford.edu/manning},
hp_SuryaGanguli={https://ganguli-gang.stanford.edu/surya.html},
hp_PercyLiang={https://cs.stanford.edu/%7epliang},
hp_NoahGoodman={cocolab.stanford.edu/ndg},
hp_FrankRichter={https://www.english-linguistics.de/fr},
hp_DetmarMeurers={https://www.sfs.uni-tuebingen.de/%7edm},
hp_MarcoBaroni={https://marcobaroni.org},
hp_YonatanBisk={https://yonatanbisk.com},
hp_YonatanBelinkov={https://www.cs.technion.ac.il/%7ebelinkov},
hp_HowardStraubing={http://www.cs.bc.edu/%7estraubin},
hp_AndreasKrebs={https://uni-tuebingen.de/pt/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/theoretische-informatik/home/mitarbeiter/dr-andreas-krebs/},
hp_Xue-XinWei={https://neuroscience.utexas.edu/component/cobalt/item/17-neuroscience/4128-wei-xuexin?Itemid=1258},
hp_NavinGoyal={https://www.microsoft.com/en-us/research/people/navingo/}
}

