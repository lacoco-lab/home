---
title: 'Tabula nearly rasa: Probing the linguistic knowledge of character-level neural
  language models trained on unsegmented text'
authors:
- Michael Hahn
- Marco Baroni
date: '2019-01-01'
publishDate: '2023-11-01T19:18:58.596531Z'
publication_types:
- article-journal
publication: '*Transactions of the Association for Computational Linguistics*'
abstract: Recurrent neural networks (RNNs) have reached striking performance in many
  natural language processing tasks. This has renewed interest in whether these generic
  sequence processing devices are inducing genuine linguistic knowledge. Nearly all
  current analytical studies, however, initialize the RNNs with a vocabulary of known
  words, and feed them tokenized input during training. We present a multi-lingual
  study of the linguistic knowledge encoded in RNNs trained as character-level language
  models, on input data with word boundaries removed. These networks face a tougher
  and more cognitively realistic task, having to discover any useful linguistic unit
  from scratch based on input statistics. The results show that our \"near tabula
  rasa\" RNNs are mostly able to solve morphological, syntactic and semantic tasks
  that intuitively presuppose word-level knowledge, and indeed they learned, to some
  extent, to track word boundaries. Our study opens the door to speculations about
  the necessity of an explicit, rigid word lexicon in language learning and usage.
links:
- name: URL
  url: https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00283
---
